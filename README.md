tf-keras-vis
===

[![Downloads](https://pepy.tech/badge/tf-keras-vis)](https://pepy.tech/project/tf-keras-vis)
[![PyPI version](https://badge.fury.io/py/tf-keras-vis.svg)](https://badge.fury.io/py/tf-keras-vis)
[![Python package](https://github.com/keisen/tf-keras-vis/actions/workflows/python-package.yml/badge.svg)](https://github.com/keisen/tf-keras-vis/actions/workflows/python-package.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


News
---

**NOTE:** We've released `v0.7.0`! In this release, fixing some critical bugs of `ActivationMaximization` and `Regularizer`, the calculation of gradients are now more stable, and some known issues related to mixed-precision are also fixed. This is good news for users using mixed-precision mechanisms and users tuning the hyper parameters of `ActivationMaximization` for their own model from now on.

However, because the calculated values of `ActivationMaximization` are now a bit different compared to the past versions, this release might badly affect users who already have the code that was adjusted to the past `ActivationMaximization`.

So please confirm [the release note](https://github.com/keisen/tf-keras-vis/releases/tag/v0.7.0), and then consider carefully whether you update tf-keras-vis in your environment.
If you have faced any problem related to this release, please feel free to ask us in [Issues page](https://github.com/keisen/tf-keras-vis/issues)!


Overview
---

tf-keras-vis is a visualization toolkit for debugging `tf.keras.Model` in Tensorflow2.0+.
Currently supported methods for visualization include:

* Feature Visualization
   - ActivationMaximization
* Class Activation Maps
   - [GradCAM](https://arxiv.org/pdf/1610.02391v1.pdf)
   - [GradCAM++](https://arxiv.org/pdf/1710.11063.pdf)
   - [ScoreCAM](https://arxiv.org/pdf/1910.01279.pdf)
   - [Faster-ScoreCAM](https://github.com/tabayashi0117/Score-CAM/blob/master/README.md#faster-score-cam)
* Saliency Maps
   - [Vanilla Saliency](https://arxiv.org/pdf/1312.6034.pdf)
   - [SmoothGrad](https://arxiv.org/pdf/1706.03825.pdf)

tf-keras-vis is designed to be light-weight, flexible and ease of use.
All visualizations have the features as follows:

* Support **N-dim image inputs**, that's, not only support pictures but also such as 3D images.
* Support **batch wise** processing, so, be able to efficiently process multiple input images.
* Support the model that have either **multiple inputs** or **multiple outputs**, or both.
* Support the **mixed-precision** model.

And in ActivationMaximization,

* Support Optimizers that are built to tf.keras.


Visualizations
---

### Visualizing Dense Layer

<img src='https://github.com/keisen/tf-keras-vis/raw/master/examples/images/visualize-dense-layer.png' width='600px' />

### Visualizing Convolutional Filer

<img src='https://github.com/keisen/tf-keras-vis/raw/master/examples/images/visualize-filters.png' width='600px' />

### GradCAM

<img src='https://github.com/keisen/tf-keras-vis/raw/master/examples/images/gradcam_plus_plus.png' width='600px' />

The images above are generated by `GradCAM++`.

### Saliency Map

<img src='https://github.com/keisen/tf-keras-vis/raw/master/examples/images/smoothgrad.png' width='600px' />

The images above are generated by `SmoothGrad`.


Usage
---

* ActivationMaximization

```python
from matplotlib import pyplot as plt
from tf_keras_vis.activation_maximization import ActivationMaximization
from tf_keras_vis.activation_maximization.callbacks import Progress
from tf_keras_vis.activation_maximization.input_modifiers import Jitter, Rotate
from tf_keras_vis.activation_maximization.regularizers import TotalVariation2D, Norm
from tf_keras_vis.utils.model_modifiers import ReplaceToLinear
from tf_keras_vis.utils.scores import CategoricalScore

# Create the visualization instance.
# All visualization classes accept a model and model-modifier, which, for example,
#     replaces the activation of last layer to linear function so on, in constructor.
activation_maximization = ActivationMaximization(YOUR_MODEL_INSTANCE,
                                                 model_modifier=ReplaceToLinear(),
                                                 clone=True)

# You can use Score class to specify visualizing target you want.
# And add regularizers or input-modifiers as needed.
activations = \
   activation_maximization(CategoricalScore(20),
                           steps=200,
                           input_modifiers=[Jitter(jitter=16), Rotate2D(degree=1)],
                           regularizers=[TotalVariation2D(weight=1.0),
                                         Norm(weight=0.3, p=1)],
                           optimizer=tf.keras.optimizers.RMSprop(1.0, 0.999),
                           callbacks=[Progress()])

## Since v0.6.0, calling `astype()` is NOT necessary.
# activations = activations[0].astype(np.uint8)

# Render
plt.imshow(activations[0])
```

Please see the guides below for more details:

### Getting Started Guides

* [Saliency and CAMs](https://github.com/keisen/tf-keras-vis/blob/master/examples/attentions.ipynb)
* [Visualize Dense Layer](https://github.com/keisen/tf-keras-vis/blob/master/examples/visualize_dense_layer.ipynb)
* [Visualize Convolutional Filer](https://github.com/keisen/tf-keras-vis/blob/master/examples/visualize_conv_filters.ipynb)

**[NOTE]**
If you have ever used [keras-vis](https://github.com/raghakot/keras-vis), you may feel that tf-keras-vis is similar with keras-vis.
Actually tf-keras-vis derived from keras-vis, and both provided visualization methods are almost the same.
But please notice that tf-keras-vis APIs does NOT have compatibility with keras-vis.


Requirements
---

* Python 3.6-3.9
* tensorflow>=2.0.4


Installation
---

* PyPI

```bash
$ pip install tf-keras-vis tensorflow
```

* Source (for development)

```bash
$ git clone https://github.com/keisen/tf-keras-vis.git
$ cd tf-keras-vis
$ pip install -e .[develop]
```


Use Cases
---

* [chitra](https://github.com/aniketmaurya/chitra)
   * A Deep Learning Computer Vision library for easy data loading, model building and model interpretation with GradCAM/GradCAM++.


Known Issues
---

* With InceptionV3, ActivationMaximization doesn't work well, that's, it might generate meaninglessly blur image.
* With cascading model, Gradcam and Gradcam++ don't work well, that's, it might occur some error. So we recommend to use FasterScoreCAM in this case.
* `channels-first` models and data is unsupported.


ToDo
---

* Guides
   * Visualizing multiple attention or activation images at once utilizing batch-system of model
   * Define various score functions
   * Visualizing attentions with multiple inputs models
   * Visualizing attentions with multiple outputs models
   * Advanced score functions
   * Tuning Activation Maximization
   * Visualizing attentions for N-dim image inputs
*  Publish API documentations as a website
*  We're going to add some methods such as below.
   - Deep Dream
   - Style transfer
